{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 출력층 설계 (Output layer)\n",
    "\n",
    "\n",
    "**회귀 vs 분류 출력층 비교표**\n",
    "\n",
    "\n",
    "| 항목           | **회귀 (Regression)**            | **이진 분류 (Binary Classification)**        | **다중 분류 (Multi-class Classification)** |\n",
    "| ------------ | ------------------------------ | ---------------------------------------- | -------------------------------------- |\n",
    "| **출력층 뉴런 수** | 1개                             | 1개                                       | 클래스 수만큼 (예: 3개 클래스 → 3개 뉴런)            |\n",
    "| **활성화 함수**   | 없음 (`Identity` = 항등함수)         | `Sigmoid`                                | 없음 (출력은 로짓값, softmax는 loss 내부 처리)      |\n",
    "| **손실 함수**    | `MSELoss`, `L1Loss` 등          | `BCELoss`, `BCEWithLogitsLoss`           | `CrossEntropyLoss` (Softmax 포함)        |\n",
    "| **정답 레이블**   | 실수 (float32), shape = `(n, 1)` | 0 또는 1 (float or long), shape = `(n, 1)` | 정수 (long), shape = `(n,)`              |\n",
    "| **예측 방식**    | 그대로 출력 사용 (`ŷ`)                | `ŷ >= 0.5` → 1, else 0                   | `argmax(output, dim=1)`                |\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "> 회귀는 **출력값에 제한이 없으므로** 아무 활성화도 적용하지 않음\n",
    ">\n",
    "> 이진 분류는 **확률**을 출력해야 하므로 sigmoid를 씌움. `BCEWithLogitsLoss` 사용하는 경우에는 **출력층에서는 sigmoid를 쓰지 않음**\n",
    ">\n",
    "> 다중 분류는 `CrossEntropyLoss`가 내부적으로 `Softmax` + `Log`를 처리하므로 **출력층에서는 softmax를 쓰지 않음**\n"
   ],
   "id": "6d30a3671985311b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 회귀 출력층\n",
    "항등함수란? $f(x) = x$와 같이 입력이 곧 출력인 함수를 가리킨다.\n",
    "torch모델에서는 출력층 다음에 아무 활성화 함수를 사용하지 않는다."
   ],
   "id": "5a299e8ddc16408c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import filelock\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn.functional import threshold"
   ],
   "id": "5eb35763ae11da00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 데이터 생성\n",
    "# X (100, 2)\n",
    "# y (100, 1)\n",
    "\n",
    "X = torch.randn(100, 2)\n",
    "W = torch.tensor([[3., 2.]]) # (1, 2)\n",
    "b = torch.tensor([5.])\n",
    "noise = torch.randn(100, 1) * 2\n",
    "\n",
    "\n",
    "y = X @ W.T + b + noise\n",
    "# ==> (100, 2) @ (1, 2) 라서 내적이 안되므로 전치해준다\n",
    "\n",
    "\n",
    "print(y)\n",
    "print(y.shape)\n",
    "# => torch.Size([100, 1])\n",
    "\n",
    "\n"
   ],
   "id": "f64709205c722efd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 생성\n",
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.hiiden = nn.Linear(input_dim, 10) # 입력 2 -> 출력 10\n",
    "        # ==> input_dim : 입력 특성 수에 따라서 바뀌게 됨\n",
    "        self.relu = nn.ReLU() # ==> 활성화 함수\n",
    "        self.output = nn.Linear(10,1) # 입력 10 -> 출력 1\n",
    "        # ==> 회귀 이므로 1개 고정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hiiden(x) # ==> 은닉층\n",
    "        x = self.relu(x)   # ==>\n",
    "        x = self.output(x) # ==> 출력층\n",
    "        return x\n",
    "\n",
    "model = RegressionNet(input_dim=X.size(1))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n"
   ],
   "id": "55efe54a2ab36bdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 학습\n",
    "for epoch in range(1000):\n",
    "    model.train() # 학습모드\n",
    "    optimizer.zero_grad() # ==> optimizer 초기화\n",
    "    pred = model(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 가중치 갱신\n",
    "\n",
    "    if (epoch + 1) % 100 == 0 :\n",
    "        print(f'Epoch : {epoch + 1} / Loss : {loss.item()}')\n"
   ],
   "id": "d6376e14b9b5b35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 예측 / 시각화\n",
    "\n",
    "# torch.no_grad() 블럭 : 자동미분 연산 안하는 블럭(loss.backward() 가용 불가)\n",
    "# model.eval() : 모델의 평가모드 활성화 (Dropout 처리)\n",
    "# 드랍아웃 : 학습 중에 일부 뉴런을 랜덤으로 꺼서 모델이 특정 뉴런에만 의존하지 못하게 만드는 과적합 방지 기법\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred = model(X)\n",
    "\n",
    "plt.scatter(y, pred)\n",
    "plt.plot(\n",
    "    [y.min(), y.max()],\n",
    "    [y.min(), y.max()],\n",
    "    'r--', label='True') # ==> 정답 선\n",
    "plt.xlabel('Actual') # ==> 실제 값\n",
    "plt.ylabel('Prediction') # ==> 예측값?\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ],
   "id": "88a70a06c1bab6d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 은닉층 가중치/절편 shape : (10, 2) (10, )\n",
    "# 출력층 가중치/절편 shape : (1, 10) (1, )\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name} : {param.shape}')\n"
   ],
   "id": "b96c10b0264ee53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bfe0284915c9706a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 캘리포니아 집값 예측",
   "id": "1e9018da6803563d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T01:57:35.353323Z",
     "start_time": "2026-01-08T01:57:35.185542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# 데이터 전처리\n",
    "X_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler() # 딥러닝은 학습안정성을 이유로 라벨 스케일링도 진행\n",
    "\n",
    "X = X_scaler.fit_transform(X)\n",
    "y = y_scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42) # ==> 회귀이므로 st뭐시기 없음\n",
    "\n",
    "# - tensor 변환\n",
    "# X_train = torch.tensor(X_train)\n",
    "# X_test = torch.tensor(X_test)\n",
    "# y_train = torch.tensor(y_train)\n",
    "# y_test = torch.tensor(y_test)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n"
   ],
   "id": "67536c237bd50b98",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 생성\n",
    "\n",
    "class CaliforniaHousingNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.hiiden1 = nn.Linear(input_dim, 32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(32, 16)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.output = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hiiden1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n"
   ],
   "id": "783b4cb2a917c97d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 생성\n",
    "\n",
    "class CaliforniaHousingNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # 모듈 겍체를 순서대로 묶고 실행\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = CaliforniaHousingNet(input_dim=X.shape[1])\n",
    "# ==> X 가 데이터프레임이므로 X.shape(1)이 아니라 X.shape[1]\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ],
   "id": "f9f7cfffbb42e547"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 모델 학습\n",
    "\n",
    "# 학습모드 활성화\n",
    "model.train()\n",
    "\n",
    "# epoch수 만큼 반복\n",
    "for epoch in range(1000) :\n",
    "    # 최적화함수 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 모델 예측\n",
    "    pred = model(X_train)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = criterion(pred, y_train)\n",
    "\n",
    "    # 역전파(기울기 계싼)\n",
    "    loss.backward()\n",
    "\n",
    "    # 가중치/절편 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 손실 로깅\n",
    "    if (epoch + 1) % 100 == 0 :\n",
    "        print(f'Epoch : {epoch + 1} / Loss : {loss.item()}')\n",
    "\n",
    "# => RuntimeError: mat1 and mat2 must have the same dtype,\n",
    "#                  but got Double and Float\n",
    "print(X_train.dtype) # ==> torch.float64 이 들엉라서"
   ],
   "id": "891cba2fcbfd629a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 학습\n",
    "\n",
    "# 학습모드 활성화\n",
    "model.train()\n",
    "\n",
    "# epoch수 만큼 반복\n",
    "for epoch in range(1000) :\n",
    "    # 최적화함수 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 모델 예측\n",
    "    pred = model(X_train)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = criterion(pred, y_train)\n",
    "\n",
    "    # 역전파(기울기 계싼)\n",
    "    loss.backward()\n",
    "\n",
    "    # 가중치/절편 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 손실 로깅\n",
    "    if (epoch + 1) % 100 == 0 :\n",
    "        print(f'Epoch : {epoch + 1} / Loss : {loss.item()}')\n"
   ],
   "id": "7d56d79bc684e43a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error, mean_absolute_error, accuracy_score\n",
    "\n",
    "# 평가/시각화\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad() :\n",
    "    pred = model(X_test)\n",
    "# pred\n",
    "\n",
    "# 라벨 스케일링값을 원복\n",
    "y_test_inv = y_scaler.inverse_transform(y_test)\n",
    "pred_inv = y_scaler.inverse_transform(pred)\n",
    "\n",
    "# 평가\n",
    "print(f'R^2 : {r2_score(y_test_inv, pred_inv)}')\n",
    "print(f'MSE : {mean_squared_error(y_test_inv, pred_inv)}')\n",
    "print(f'MAE : {mean_absolute_error(y_test_inv, pred_inv)}')\n",
    "print(f'RMSE : {root_mean_squared_error(y_test_inv, pred_inv)}')"
   ],
   "id": "7e91ab4a0c681a36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 시각화\n",
    "# - 산점도 : x축 실제값, y축 실제값\n",
    "# - 선그래프(기준선) : (0, 0) -> (최대실제값, 최대실제값)\n",
    "\n",
    "plt.scatter(y_test_inv, pred_inv)\n",
    "plt.plot(\n",
    "    [0, y_test_inv.max()], [0, y_test_inv.max()],\n",
    "    'r--', label='True'\n",
    ")\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Prediction')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "id": "474cc8c6644c0601"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 생성\n",
    "\n",
    "class CaliforniaHousingNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # 모듈 겍체를 순서대로 묶고 실행\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = CaliforniaHousingNet(input_dim=X.shape[1])\n",
    "# ==> X 가 데이터프레임이므로 X.shape(1)이 아니라 X.shape[1]\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ],
   "id": "5a58cc0ce8f1d130"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 이진분류\n",
    "- Sigmoid 활성화 함수 사용\n",
    "- 은닉층/출력층을 거쳐온 결과값(z)를 확률값(p)으로 변환\n",
    "- 설정한 임계치 이상이면 양성으로 예측"
   ],
   "id": "1d3216dca41e2efe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "torch.manual_seed(42)\n",
    "# ==> 결과값 고정\n",
    "\n",
    "# 평균 0, 표준편차 10인 정규분포 샘플링 10개\n",
    "z = torch.randn(10) * 10\n",
    "# print(f'z : {z}')\n",
    "\n",
    "p = F.sigmoid(z)\n",
    "# print(f'p : {p}')\n",
    "\n",
    "# 더 많은 양성클래스를 확보하고 싶다면(재현율), 임계치를 낮추면 된다\n",
    "# 양성클래스의 정밀도를 높이고 싶다면, 임계치를 높이면 된다.\n",
    "threshold = 0.5 # ==> 임계치\n",
    "pred = (p >= threshold).int() # ==> 값계산 + 형변환\n",
    "# pred\n",
    "# => tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 1], dtype=torch.int32)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'z' : z,\n",
    "    'p' : p,\n",
    "    'pred' : pred,\n",
    "})\n",
    "\n"
   ],
   "id": "85a5e726a45142d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 다중분류\n",
    "- Softmax 활성화함수 사용\n",
    "- 각 클래스별 계산값을 입력으로 받아, 각 클래스별 확률값으로 변환 (모든 클래스의 확률값 합 1)\n",
    "- 벡터를 입력받아 벡터로 변환"
   ],
   "id": "9983e72cfe5b5d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 데이터샘플이 한건인 경우\n",
    "z = torch.tensor([2., 1.5, 4, 0.7])\n",
    "print(z)\n",
    "output = F.softmax(z, dim=0)\n",
    "print(output)\n",
    "\n"
   ],
   "id": "f206330950b0f431"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 데이터샘플이 여러 건인 경우\n",
    "z = torch.tensor([[2., 1.5, 4, 0.7],\n",
    "                  [3., 1, 4.7, 5]])\n",
    "print(z)\n",
    "output = F.softmax(z, dim=1)\n",
    "print(output)\n",
    "# => tensor([[2.0000, 1.5000, 4.0000, 0.7000],\n",
    "#            [3.0000, 1.0000, 4.7000, 5.0000]])\n",
    "# => tensor([[0.1079, 0.0654, 0.7973, 0.0294],\n",
    "#            [0.0714, 0.0097, 0.3910, 0.5279]])\n",
    "\n",
    "pred = output.argmax(dim=1)\n",
    "print(pred)\n",
    "# => tensor([2, 3])\n",
    "\n",
    "\n",
    "# softmax 결과의 합은 항상 1\n",
    "print(output.sum(dim=1))\n",
    "# => tensor([1.0000, 1.0000])\n"
   ],
   "id": "19c17b9cff389f40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# (참고) 다중클래스 예측에 sigmoid를 사용하는 경우\n",
    "z = torch.tensor([[2., 1.5, 4, 0.7],\n",
    "                  [3., 1, 4.7, 5]])\n",
    "print(z)\n",
    "\n",
    "# 각 클래스별로 양성일 확률을 반환 (모두의 합이 1이 아니다)\n",
    "p = F.sigmoid(z)\n",
    "print(p)\n",
    "\n",
    "# 예측\n",
    "pred = p.argmax(dim=1)\n",
    "print(pred)"
   ],
   "id": "9a284e5f97f4ce37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 출력층과 손실함수 연계\n",
    "\n",
    "**이진분류**\n",
    "- 출력층 sigmoid + 손실함수 BCLoss\n",
    "- 출력층 x + 손실함수 BCEWithLogitsLoss\n",
    "\n",
    "\n",
    "**다중분류**\n",
    "- 출력층 x + 손실함수 CrossEntropyLoss"
   ],
   "id": "3aa69753d3e0bb38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 이진분류 : 출력층 sigmoid + 손실함수 BECLoss",
   "id": "8d822bded53e3e48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.datasets import  make_classification\n",
    "\n",
    "# 데이터 생성\n",
    "X, y = make_classification(\n",
    "    n_samples=100, # 데이터 수\n",
    "    n_features=10, # 특성 수\n",
    "    n_informative=5, # 중요한 속성 수\n",
    "    n_classes=2, # 예측클래스 (이진분류)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "# => (100, 10) (100,)\n",
    "\n",
    "# 데이터 준비 (tensor 타입)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1) # 2차원 변경\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "# => torch.Size([100, 10]) torch.Size([100, 1])"
   ],
   "id": "d272b3f933664fbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 생성\n",
    "\n",
    "class BinaryClassificationNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ==> model, criterion, optimizer 준비\n",
    "model = BinaryClassificationNet(input_dim=X.shape[1]) # 입력데이터 특성수\n",
    "# => 10\n",
    "criterion = nn.BCELoss() # 출력층 sigmoid가 반환한 확률값을 가지고 손실 계산\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ],
   "id": "24579783df2192c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 학습\n",
    "epochs = 100\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X)\n",
    "    # ==> 예측해봐\n",
    "    loss = criterion(pred, y) # 100개 샘플 오차의 평균값(스칼라)\n",
    "    # ==> 보통 평균이나 합계 단일값으로 함\n",
    "    # ==> 손실 계산해봐\n",
    "    loss.backward()\n",
    "    # ==> 손실 줄이게 기울기 계산해봐\n",
    "    optimizer.step()\n",
    "    # ==> 그걸고 가중치 업데이트 해봐?\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 :\n",
    "        print(f'Epoch : {epoch + 1} / Loss : {loss.item()}')\n"
   ],
   "id": "2b2f59e9ce554407"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 출력층 x + 손실함수 BCEWithLogitLoss",
   "id": "ca6148e9306de285"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 생성\n",
    "\n",
    "class BinaryClassificationNet2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x) # sigmoid 활성화함수를 사용하지 않으므로,\n",
    "                           # 확률값이 아닌 선형방정식 값이 반환\n",
    "\n",
    "# ==> model, criterion, optimizer 준비\n",
    "model = BinaryClassificationNet2(input_dim=X.shape[1]) # 입력데이터 특성수\n",
    "criterion = nn.BCEWithLogitsLoss() # 내부적으로 sigmoid 활성화함수 처리\n",
    "# ==> 주의!! : sigmoid를 두번 돌리지 않게 조심하기\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ],
   "id": "dae04660ec18358a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 학습\n",
    "epochs = 100\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X)\n",
    "    loss = criterion(logits, y) # 100개 샘플 오차의 평균값(스칼라)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 :\n",
    "        print(f'Epoch : {epoch + 1} / Loss : {loss.item()}')\n"
   ],
   "id": "177797d06a98e5a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "유방암 예측",
   "id": "a860c387b69cd7dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T01:36:14.944788Z",
     "start_time": "2026-01-13T01:36:10.498802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import  load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "print(X.shape, y.shape)\n",
    "# => (569, 30) (569,)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 데이터 전처리\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# ==> 이진분류이므로 y 전처리 없음\n",
    "print(X_train.dtype) # float64\n",
    "\n",
    "\n",
    "# Tensor 데이터 준비\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
    "# ==> unsqueeze로 차원 늘리기\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "# => torch.Size([455, 30]) torch.Size([455])\n",
    "# => torch.Size([114, 30]) torch.Size([114])\n",
    "# ===> unsqueeze 후\n",
    "# => torch.Size([455, 30]) torch.Size([455, 1])\n",
    "# => torch.Size([114, 30]) torch.Size([114, 1])"
   ],
   "id": "b8d12aa57fefea8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 작성\n",
    "class BreastCancerNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "# 모델 / 손실함수 / 최적화함수 선언\n",
    "model = BreastCancerNet(input_dim=X_train.size(1))\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ],
   "id": "e30a3cb164b6f775"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 학습\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100) :\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X_train) # 로짓\n",
    "    # ==> 변수명은 pred지만 sigmoid가 없으므로 실제 들어오는건 logits값 이다\n",
    "    loss = criterion(pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 :\n",
    "        print(f'Epoch : {epoch + 1} / Loss : {loss.item()}')\n"
   ],
   "id": "e4c640a06c459e75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "with torch.no_grad() : # ==> 기울기 계산할 필요 없다\n",
    "    logits = model(X_test)\n",
    "    # print(logits) # ==> 로짓값이 나온다\n",
    "    p = F.sigmoid(logits)\n",
    "    # print(p) # ==> 0에서 1 사이 값 나온다\n",
    "    pred = (p >= 0.5).int()\n",
    "    # print(pred) # ==> 이진분류 (0 또는 1)값으로 나온다\n",
    "    print('정확도 : ', accuracy_score(y_test, pred))\n"
   ],
   "id": "55bc1dfca714aad8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 출력층 x + 손실함수 CrossEntropyLoss",
   "id": "552a3b94bf4aa118"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 데이터 생성\n",
    "X = torch.randn(4, 5) # 정규분포 4개의 데이터샘플, 5개의 특성\n",
    "# ==> 총 20개\n",
    "y = torch.tensor([0, 2, 1, 0]) # 라벨 자료형 long\n",
    "n_classes = len(y.unique())\n",
    "# n_classes\n",
    "# => 3\n",
    "\n",
    "# 모델 생성\n",
    "class MultiClassficationNet(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32), # 은닉층\n",
    "            nn.ReLU(), # 은닉층 활성화함수\n",
    "            # nn.Linear(32, 3) # 출력층\n",
    "            # ==> 다중분류 정답이 3개이므로 노드 3개\n",
    "            nn.Linear(32, n_classes) # 출력층\n",
    "            # ==> 정답 개수가 바뀔 수 있으므로 변수화\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MultiClassficationNet(input_dim=X.size(1), n_classes=n_classes)\n",
    "criterion = nn.CrossEntropyLoss() # softmax 내장\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ],
   "id": "53c67f4c1e60f20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 학습\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100) :\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X) # 클래스별 로짓\n",
    "    # print(logits.shape)\n",
    "    loss = criterion(logits, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 :\n",
    "        print(f'Epoch : {epoch + 1} / Loss : {loss.item()}')\n"
   ],
   "id": "40b6af50a9e8992a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 붓꽃데이터 예측",
   "id": "872a7f161333f0b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape, y.shape)\n",
    "# => (150, 4) (150,)\n",
    "\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.dtype)\n",
    "# => float64\n",
    "\n",
    "# Tensor 변환\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(y_train.dtype)\n",
    "# => torch.int64\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "# => torch.Size([120, 4]) torch.Size([120])\n",
    "# => torch.Size([30, 4]) torch.Size([30])"
   ],
   "id": "38d151b5ab3dbf8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 모델 생성\n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = IrisNet(input_dim=X_train.size(1), output_dim=3)\n",
    "# ==> y_train에서 unique값 꺼내서 output_dim에 넣어도 상관없음\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # softmax 내장\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 모델 학습\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100) :\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(X_train)\n",
    "    # print(logits) # ==> 모르겠으면 찍어봐라\n",
    "\n",
    "    loss = criterion(logits, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 :\n",
    "        print(f'Epoch : {epoch + 1} / Loss : {loss.item()}')\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "with torch.no_grad() :\n",
    "    logits = model(X_test)\n",
    "    print('logits : ', logits[:2])\n",
    "    p = F.softmax(logits, dim=1)\n",
    "    print('p : ', p[:2])\n",
    "    pred = p.argmax(dim=1) # 열 고정 : 행간 가장 큰 확률값의 인덱스\n",
    "    # ==> dim=1 행별로 놓고 봤을 때 가장 큰 확률값의 인덱스\n",
    "    print('pred : ', pred[:2])\n",
    "    print('y_true : ', y_test[:2])\n",
    "\n",
    "    print('정확도 : ', accuracy_score(y_test, pred))\n",
    "# => logits :  tensor([[  7.0337,  -2.9967, -11.4675],\n",
    "#                      [ -5.4418,   1.2109,   2.4510]])\n",
    "# =>      p :  tensor([[9.9996e-01, 4.4038e-05, 9.2256e-09],\n",
    "#                      [2.8955e-04, 2.2435e-01, 7.7536e-01]])\n",
    "# =>   pred :  tensor([0, 2])\n",
    "# => y_true :  tensor([0, 2])\n",
    "# =>  정확도 :  0.9666666666666667"
   ],
   "id": "1a84000e096ef29b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
