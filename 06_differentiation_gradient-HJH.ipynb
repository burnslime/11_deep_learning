{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 01. 미분\n",
    "\n",
    "\n",
    "미분의 기본 개념은 함수의 변화율을 구하는 데 있다. 이는 함수가 입력의 변화에 따라 얼마나 변하는지를 수학적으로 분석하는 도구이다.\n",
    "\n",
    "\n",
    "### 01-01. 평균 변화율\n",
    "\n",
    "\n",
    "먼저, 함수 $f(x)$의 **평균 변화율**을 살펴본다. 이는 두 점 $x$와 $x + h$ 사이에서 함수 $f(x)$가 얼마나 변했는지를 나타내는 지표이다.\n",
    "\n",
    "\n",
    "$$\n",
    "평균변화율 = \\frac{f(x + h) − f(x)}{h}\n",
    "$$\n",
    "\n",
    "\n",
    "예를 들어, $f(x) = 2x$라 할 때, $x=5$에서 $x=8$로 바뀌었다고 하면, 평균 변화율은 다음과 같다.\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{f(8) - f(5)}{3} = \\frac{16 - 10}{3} = 2\n",
    "$$\n",
    "\n",
    "\n",
    "이는 $h$만큼의 변화에 대해 함수 $f$가 얼마나 변하는지를 나타내며, 기울기의 개념과도 연결된다.\n",
    "\n",
    "\n",
    "### 01-02. 순간 변화율 (미분의 정의)\n",
    "\n",
    "\n",
    "**순간 변화율**은 평균 변화율에서 $h$가 0에 극한으로 가까워질 때를 의미한다. 이를 통해 미분을 정의할 수 있다.\n",
    "\n",
    "\n",
    "$$\n",
    "f′(x) = \\lim_{h→0}\\frac{f(x + h) − f(x)}{h}\n",
    "$$\n",
    "\n",
    "\n",
    "**예: $f(x) = x^2$의 미분**\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{(x + h)^2 − x^2}{h} = \\frac{x^2 + 2xh + h^2 − x^2}{h} = \\frac{2xh + h^2}{h} = 2x + h\n",
    "$$\n",
    "\n",
    "\n",
    "극한을 취하면 다음과 같다.\n",
    "\n",
    "\n",
    "$$\n",
    "\\lim_{h → 0}(2x + h) = 2x\n",
    "$$\n",
    "\n",
    "\n",
    "따라서, $f(x) = x^2$의 미분은 $f′(x) = 2x$이다.\n",
    "\n",
    "\n",
    "### 01-03. 다양한 미분 예시\n",
    "\n",
    "\n",
    "#### 01-03-01. 다항 함수 $f(x) = x^2$\n",
    "\n",
    "\n",
    "위와 같이 전개 및 극한을 통해 다음을 얻는다.\n",
    "\n",
    "\n",
    "$$\n",
    "f′(x) = 2x\n",
    "$$\n",
    "\n",
    "\n",
    "#### 01-03-02. 상수 함수 $f(x) = c$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{f(x + h) − f(x)}{h} = \\frac{c − c}{h} = 0\n",
    "$$\n",
    "\n",
    "\n",
    "상수 함수는 입력 변화에 관계없이 값이 일정하므로 미분값은 항상 0이다.\n",
    "\n",
    "\n",
    "$$\n",
    "f′(x) = 0\n",
    "$$\n",
    "\n",
    "\n",
    "#### 01-03-03. 선형 함수 $f(x) = mx + b$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{f(x + h) − f(x)}{h} = \\frac{m(x + h) + b − (mx + b)}{h} = \\frac{mh}{h} = m\n",
    "$$\n",
    "\n",
    "\n",
    "따라서, 선형 함수의 미분은 $f′(x) = m$이다.\n",
    "\n",
    "\n",
    "### 01-04. 미분의 기본 규칙\n",
    "\n",
    "\n",
    "1. 상수 $c$의 미분은 $0$이다.\n",
    "2. $f(x) = x^n$ 형태의 미분은 $nx^{n−1}$이다.\n",
    "3. 함수의 선형 조합 $af(x) + bg(x)$의 미분은 $af′(x) + bg′(x)$이다.\n",
    "\n",
    "\n",
    "### 01-05. 미분 표기\n",
    "\n",
    "\n",
    "라이프니츠는 미분을 다음과 같은 표기로 정의하였다:\n",
    "\n",
    "\n",
    "* $d$: differential, 미소한 변화\n",
    "* $dx$: $x$의 미소 변화\n",
    "* $\\frac{d}{dx}$: $x$에 대한 미분 연산자\n",
    "\n",
    "\n",
    "**연쇄 법칙 표기**\n",
    "\n",
    "\n",
    "합성 함수 $z = f(g(x))$에 대해 다음과 같이 연쇄법칙을 적용할 수 있다:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "\n",
    "### 01-06. 미분의 연쇄 법칙\n",
    "\n",
    "\n",
    "합성 함수 $y = f(g(x))$의 미분은 내부 함수와 외부 함수의 미분을 곱하여 구한다.\n",
    "\n",
    "\n",
    "* $y = f(u)$\n",
    "* $u = g(x)$\n",
    "\n",
    "\n",
    "이때:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    "\n",
    "**예시**\n",
    "\n",
    "\n",
    "$z = y^2$, $y = 5x + 2$인 경우,\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} = 2y \\cdot 5 = 10y\n",
    "$$\n",
    "\n",
    "\n",
    "$y = 5x + 2$이므로:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = 10(5x + 2) = 50x + 20\n",
    "$$\n",
    "\n",
    "\n",
    "이는 직접 미분한 결과와 동일하다.\n"
   ],
   "id": "6fbcb85a25a6a70c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 02. 미분과 수치미분 개념\n",
    "\n",
    "\n",
    "**수학적 미분 (Analytical Differentiation)**\n",
    "\n",
    "\n",
    "$$\n",
    "f(x) = x^2 \\Rightarrow f'(x) = 2x\n",
    "$$\n",
    "\n",
    "\n",
    "* 수학적으로 정확한 **도함수**를 구함\n",
    "* 함수의 변화율을 계산하는 공식 기반 미분\n",
    "\n",
    "\n",
    "**수치미분 (Numerical Differentiation)**\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\n",
    "$$\n",
    "* 도함수를 직접 구하지 않고, **근사값으로 기울기를 계산**\n",
    "* 여기서 $h$는 아주 작은 수 (예: $h = 1e-4$)\n",
    "* 실제 계산 과정을 코드로 볼 수 있어 **직관적**\n",
    "\n",
    "\n",
    "**미분 vs 수치미분 비교표**\n",
    "\n",
    "\n",
    "| 항목    | 수학적 미분 (Analytical) | 수치미분 (Numerical)          |\n",
    "| ----- | ------------------- | ------------------------- |\n",
    "| 정의 방식 | 공식에 따라 도함수 계산       | 근사값으로 기울기 계산              |\n",
    "| 정확도   | 매우 정확함              | 근사값, 오차 발생 가능             |\n",
    "| 속도    | 빠름                  | 느림 (함수값 여러 번 계산 필요)       |\n",
    "| 용도    | 모델 학습, 역전파 계산       | 디버깅, 검증용 (Gradient Check) |\n",
    "\n",
    "\n",
    "**수치미분 방식비교**\n",
    "\n",
    "\n",
    "> 중앙차분(Central Difference) 방식이 가장 보편적으로 사용된다.\n",
    "\n",
    "\n",
    "| 방식   | 수식                                           | 정확도    | 계산 비용 | 특징         |\n",
    "| ---- | -------------------------------------------- | ------ | ----- | ---------- |\n",
    "| 전진차분 | $f'(x) \\approx \\frac{f(x+h) - f(x)}{h}$    | 1차 정확도 | 낮음    | 간단하지만 오차 큼 |\n",
    "| 후진차분 | $f'(x) \\approx \\frac{f(x) - f(x-h)}{h}$    | 1차 정확도 | 낮음    | 전진과 비슷     |\n",
    "| 중앙차분 | $f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}$ | 2차 정확도 | 높음    | 정확하고 균형잡힘  |\n"
   ],
   "id": "3b0c222f95b2bee0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T01:37:36.248661Z",
     "start_time": "2026-01-14T01:37:33.022904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import MSELoss"
   ],
   "id": "8a7588e9091c285f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T01:48:23.956865Z",
     "start_time": "2026-01-14T01:48:23.947019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# 수학적 미분\n",
    "def df(x):\n",
    "    return 2 * x\n",
    "\n",
    "# 수치미분 (중앙차분)\n",
    "def nummerical_diff(x, h=1e-4):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# 전진차분\n",
    "def nummerical_diff_forward(x, h=1e-4):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "# 후진차분\n",
    "def nummerical_diff_backward(x, h=1e-4):\n",
    "    return (f(x) - f(x - h)) / h\n",
    "\n",
    "\n",
    "x = np.array(4)\n",
    "print(f(x))\n",
    "print(df(x)) # x에 따른 f(x) 순간 기울기 (미분)\n",
    "print(nummerical_diff(x))\n",
    "print(nummerical_diff_forward(x))\n",
    "print(nummerical_diff_backward(x))"
   ],
   "id": "be928374836e12c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "7.999999999999119\n",
      "8.00009999998963\n",
      "7.9999000000086085\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 03. PyTorch의 자동미분",
   "id": "4832d01103df2511"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T01:54:24.554176Z",
     "start_time": "2026-01-14T01:54:24.545423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 연산을 시작하는 텐서\n",
    "x = torch.tensor(4. , requires_grad=True)\n",
    "# y = f(x) # x ** 2\n",
    "y = x ** 2\n",
    "\n",
    "print(y)\n",
    "print(y.grad_fn) # tensor 연산기록객체, 직접 실행불가\n",
    "y.backward() # 미분(기울기)계산이 거꾸로 처리, 계산 결과는 x.grad 속성에 저장\n",
    "\n",
    "print(x.grad)"
   ],
   "id": "494c131e30375d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16., grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x0000015AAF0599C0>\n",
      "tensor(8.)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T02:07:32.071735Z",
     "start_time": "2026-01-14T02:07:32.058349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 복잡한 연산\n",
    "x = torch.tensor([4.0], requires_grad=True) # 리프텐서 (연산그래프 시작점) 미분대상\n",
    "y = (x * 3 + 5) ** 2\n",
    "\n",
    "print(y)\n",
    "\n",
    "print(y.grad_fn)\n",
    "print(y.grad_fn.next_functions)\n",
    "\n",
    "# 연산그래프 추적 함수\n",
    "def print_grad_fn_tree(fn, indent=0):\n",
    "    # 재귀함수 종료 조건\n",
    "    if fn is None:\n",
    "        return\n",
    "\n",
    "    print(' ' * indent + f'└{fn.__class__.__name__}')\n",
    "\n",
    "    if hasattr(fn, 'next_functions'):\n",
    "        for sub, _ in fn.next_functions:\n",
    "            print_grad_fn_tree(sub, indent + 4) # 함수안에서 나를 다시 호출 = 재귀함수\n",
    "\n",
    "print_grad_fn_tree(y.grad_fn) # y = (x * 3 + 5) ** 2\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ],
   "id": "a3540e4579fb3440",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([289.], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x0000015AAF0F2A40>\n",
      "((<AddBackward0 object at 0x0000015AAF0F3130>, 0),)\n",
      "└PowBackward0\n",
      "    └AddBackward0\n",
      "        └MulBackward0\n",
      "            └AccumulateGrad\n",
      "tensor([102.])\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 04. 다변수함수 미분(편미분)",
   "id": "12d0df82708010a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T02:42:05.155141Z",
     "start_time": "2026-01-14T02:42:05.147731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 다변수 미분\n",
    "def f(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "\n",
    "# (도함수) f에 대한 x편미분\n",
    "def df_dx(x, y):\n",
    "    return 2 * x\n",
    "\n",
    "# (도함수) f에 대한 y편미분\n",
    "def df_dy(x, y):\n",
    "    return 2 * y\n",
    "\n",
    "\n",
    "# 수치미분\n",
    "def numerical_partial_diff(f, x, y, var='x', h=1e-4):\n",
    "    if var == 'x':\n",
    "        return (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "    else:\n",
    "        return (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "\n",
    "print(f(3, 4))\n",
    "print('df_dx : ', df_dx(3, 4))\n",
    "print('df_dy : ', df_dy(3, 4))\n",
    "\n",
    "print('x에 대한 수치편미분 : ', numerical_partial_diff(f, 3, 4, var='x'))\n",
    "print('y에 대한 수치편미분 : ', numerical_partial_diff(f, 3, 4, var='y'))"
   ],
   "id": "c4e3c142a3e53793",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "df_dx :  6\n",
      "df_dy :  8\n",
      "x에 대한 수치편미분 :  6.00000000000378\n",
      "y에 대한 수치편미분 :  7.999999999999119\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T02:46:36.341598Z",
     "start_time": "2026-01-14T02:46:36.332639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch 자동미분\n",
    "x = torch.tensor(3., requires_grad=True)\n",
    "y = torch.tensor(4., requires_grad=True)\n",
    "\n",
    "z = f(x, y) # x ** 2 + y ** 2\n",
    "print(z)\n",
    "print_grad_fn_tree(z.grad_fn)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad.item())\n",
    "print(y.grad.item())"
   ],
   "id": "8c7aee27aab8d39e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25., grad_fn=<AddBackward0>)\n",
      "└AddBackward0\n",
      "    └PowBackward0\n",
      "        └AccumulateGrad\n",
      "    └PowBackward0\n",
      "        └AccumulateGrad\n",
      "6.0\n",
      "8.0\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 05. 선형층 함수 형태\n",
    "* 입력: $x$ (고정)\n",
    "* 실제 정답: $y$ (고정)\n",
    "* 예측: $\\hat{y} = wx + b$\n",
    "* 손실: $L = (\\hat{y} - y)^2 = (wx + b - y)^2$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "즉, 손실 함수를\n",
    "\n",
    "\n",
    "$$\n",
    "L(w, b) = (wx + b - y)^2\n",
    "$$\n",
    "\n",
    "\n",
    "로 보고, $w$와 $b$에 대해 편미분한다.\n",
    "\n",
    "\n",
    "**수학적 미분**\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = 2(wx + b - y) \\cdot x \\\\\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = 2(wx + b - y)\n",
    "$$\n"
   ],
   "id": "1e3d8c6a55ecb161"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T03:03:01.792020Z",
     "start_time": "2026-01-14T03:03:01.784095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 선형층 + 손실 함수 구현\n",
    "def L(w, x, b, y):\n",
    "    y_hat = w * x + b\n",
    "    loss = (y_hat - y) ** 2\n",
    "    return loss\n",
    "\n",
    "def dL_dw(w, x, b, y):\n",
    "    return 2 * (w * x + b - y) * x\n",
    "\n",
    "def dL_db(w, x, b, y):\n",
    "    return 2 * (w * x + b - y)\n",
    "\n",
    "# x, y : 지도학습에 제공된 값(고정)\n",
    "# w, b : 모델이 학습할 가중치/절편 (미분대상)\n",
    "w, x, b, y = 2.0, 3.0, 1.0, 10.0\n",
    "\n",
    "print(L(w, x, b, y))\n",
    "print('L을 w로 편미분 : ', dL_dw(w, x, b, y))\n",
    "print('L을 b로 편미분 : ', dL_db(w, x, b, y))"
   ],
   "id": "a7f55e2690f232b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "L을 w로 편미분 :  -18.0\n",
      "L을 b로 편미분 :  -6.0\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T03:08:25.500771Z",
     "start_time": "2026-01-14T03:08:25.491880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(10.0)\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "loss = L(w, x, b, y)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "print('L을 w로 편미분 : ', w.grad.item())\n",
    "print('L을 b로 편미분 : ', b.grad.item())\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ],
   "id": "24af25b0442544a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<PowBackward0>)\n",
      "L을 w로 편미분 :  -18.0\n",
      "L을 b로 편미분 :  -6.0\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 06.벡터/행렬 형태\n",
    "\n",
    "\n",
    "* 예측 함수: $\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b$\n",
    "* 손실 함수: $L = (\\hat{y} - y)^2$\n",
    "* 가중치 벡터 $\\mathbf{w}$에 대한 편미분\n",
    "* NumPy 및 PyTorch 코드 비교\n",
    "\n",
    "\n",
    "**목표**\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b \\\\\n",
    "L = (\\hat{y} - y)^2 = (\\mathbf{w}^\\top \\mathbf{x} + b - y)^2\n",
    "$$\n",
    "\n",
    "\n",
    "* $\\mathbf{x}$: 입력 벡터 (예: 특징 3개짜리 입력)\n",
    "* $\\mathbf{w}$: 가중치 벡터\n",
    "* $b$: 편향\n",
    "* $y$: 정답 (스칼라)\n",
    "\n",
    "\n",
    "**수학적으로 편미분**\n",
    "\n",
    "\n",
    "손실 함수:\n",
    "\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}, b) = (\\mathbf{w}^\\top \\mathbf{x} + b - y)^2\n",
    "$$\n",
    "\n",
    "\n",
    "1. $\\frac{\\partial L}{\\partial \\mathbf{w}}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{w}} = 2(\\mathbf{w}^\\top \\mathbf{x} + b - y) \\cdot \\mathbf{x}\n",
    "$$\n",
    "\n",
    "\n",
    "2. $\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = 2(\\mathbf{w}^\\top \\mathbf{x} + b - y)\n",
    "$$\n"
   ],
   "id": "7bb7fc27e030f4b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T03:16:46.582774Z",
     "start_time": "2026-01-14T03:16:46.573397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 벡터/행렬형태의 선형층 + 손실함수\n",
    "X = np.array([[1., 2., 3.], [4., 5., 6.]]) # (2, 3)\n",
    "y = np.array([2.])\n",
    "W = np.array([[0.1, 0.2, 0.3]]) # (1, 3)\n",
    "b = np.array([0.5])\n",
    "\n",
    "def L(W, X, b, y):\n",
    "    y_hat = X @ W.T + b\n",
    "    loss = (y_hat - y) ** 2\n",
    "    return np.mean(loss)\n",
    "\n",
    "def dL_dW(W, X, b, y):\n",
    "    y_hat = X @ W.T + b\n",
    "    return np.mean(2 * (y_hat - y) * X, axis=0)\n",
    "\n",
    "def dL_db(W, X, b, y):\n",
    "    y_hat = X @ W.T + b\n",
    "    return np.mean(2 * (y_hat - y), axis=0)\n",
    "\n",
    "print(L(W, X, b, y))\n",
    "print('L에 대한 W편미분 : ', dL_dW(W, X, b, y))\n",
    "print('L에 대한 b편미분 : ', dL_db(W, X, b, y))"
   ],
   "id": "21671291ddd3975b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4499999999999997\n",
      "L에 대한 W편미분 :  [6.7 8.3 9.9]\n",
      "L에 대한 b편미분 :  [1.6]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T03:18:59.577674Z",
     "start_time": "2026-01-14T03:18:59.565382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 자동미분\n",
    "X = torch.tensor([[1., 2., 3.], [4., 5., 6.]]) # (2, 3)\n",
    "y = torch.tensor([2.])\n",
    "W = torch.tensor([[0.1, 0.2, 0.3]], requires_grad=True) # (1, 3)\n",
    "b = torch.tensor([0.5], requires_grad=True)\n",
    "\n",
    "def L(W, X, b, y):\n",
    "    y_hat = X @ W.T + b\n",
    "    loss = (y_hat - y) ** 2\n",
    "    return torch.mean(loss)\n",
    "\n",
    "loss = L(W, X, b, y)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()\n",
    "print(W.grad)\n",
    "print(b.grad)"
   ],
   "id": "5f86bbf683559070",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4500, grad_fn=<MeanBackward0>)\n",
      "tensor([[6.7000, 8.3000, 9.9000]])\n",
      "tensor([1.6000])\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 07.Gradient Descent 가중치 업데이트 방식 비교\n",
    "\n",
    "\n",
    "| 방식                | 설명                | 특징           |\n",
    "| ----------------- | ----------------- | ------------ |\n",
    "| **Batch GD**      | 전체 데이터로 한 번에 업데이트 | 가장 안정적, 느림   |\n",
    "| **Mini-Batch GD** | 일부 데이터(배치)마다 업데이트 | 일반적인 학습 방식   |\n",
    "| **SGD**           | 한 데이터마다 바로 업데이트   | 노이즈 큼, 빠른 반응 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**공통 조건**\n",
    "\n",
    "\n",
    "* 문제: 단순 선형 회귀 $y = wx + b$\n",
    "* 손실: MSE (평균제곱오차)\n",
    "* 데이터: 임의의 간단한 샘플로 구성 (2차원 입력, 100개 샘플)\n"
   ],
   "id": "cd3433a785624503"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### tensor 직접 제어\n",
    "\n",
    "- BGD\n",
    "- Mini-batch GD\n",
    "- SGD"
   ],
   "id": "c4fbc7b70283b9df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T03:44:31.491985Z",
     "start_time": "2026-01-14T03:44:31.482452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터 생성\n",
    "torch.manual_seed(42)\n",
    "\n",
    "X = torch.randn(100, 2)\n",
    "W_true = torch.tensor([[2.0, -3.0]])\n",
    "b_true = torch.tensor([5.0])\n",
    "y = X @ W_true.T + b_true + torch.randn(100, 1) * 0.5\n",
    "\n",
    "print(X.shape, y.shape)"
   ],
   "id": "9617114a0594caea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2]) torch.Size([100, 1])\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T03:54:18.421755Z",
     "start_time": "2026-01-14T03:54:18.400968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. BGD\n",
    "W = torch.zeros(1, 2, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(20):\n",
    "    y_hat = X @ W.T + b\n",
    "    loss = torch.mean((y_hat - y) ** 2) # 스칼라 값 얻을려고 평균으로 구함\n",
    "    loss.backward()\n",
    "\n",
    "    # 최적화함수 대신 직접 갱신\n",
    "    with torch.no_grad():\n",
    "        W -= lr * W.grad # loss를 줄일려면 기울기 반대로 가야함\n",
    "        b -= lr * b.grad\n",
    "\n",
    "        W.grad.zero_() # 메소드_ 형식일때는 in-place 처리 W.grad = W.grad.zero\n",
    "        b.grad.zero_()\n",
    "\n",
    "    print(f'epoch {epoch + 1} : loss {loss.item():4f}')\n"
   ],
   "id": "2881a6bbd6db1954",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : loss 26.907991\n",
      "epoch 2 : loss 19.797335\n",
      "epoch 3 : loss 14.713387\n",
      "epoch 4 : loss 11.029496\n",
      "epoch 5 : loss 8.329667\n",
      "epoch 6 : loss 6.332347\n",
      "epoch 7 : loss 4.843369\n",
      "epoch 8 : loss 3.726490\n",
      "epoch 9 : loss 2.884600\n",
      "epoch 10 : loss 2.247530\n",
      "epoch 11 : loss 1.763981\n",
      "epoch 12 : loss 1.396084\n",
      "epoch 13 : loss 1.115659\n",
      "epoch 14 : loss 0.901599\n",
      "epoch 15 : loss 0.738015\n",
      "epoch 16 : loss 0.612897\n",
      "epoch 17 : loss 0.517133\n",
      "epoch 18 : loss 0.443799\n",
      "epoch 19 : loss 0.387618\n",
      "epoch 20 : loss 0.344564\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T04:05:36.686351Z",
     "start_time": "2026-01-14T04:05:36.634716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Mini-batch 방식\n",
    "W = torch.zeros(1, 2, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "lr = 0.1\n",
    "batch_size = 20 # 100개의 샘플을 20개씩 5번 처리 (하나의 epoch당)\n",
    "\n",
    "for epoch in range(20):\n",
    "    indices = torch.randperm(X.size(0)) # X.size(0) == 100\n",
    "    # print(indices.numpy())\n",
    "\n",
    "    for i in range(0, X.size(0), batch_size):\n",
    "        batch_index = indices[i : i + batch_size]\n",
    "        # print('          ',batch_index.numpy())\n",
    "        X_batch = X[batch_index]\n",
    "        y_batch = y[batch_index]\n",
    "\n",
    "        y_hat = X_batch @ W.T + b\n",
    "        loss = torch.mean((y_hat - y_batch) ** 2)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 기울기 갱신\n",
    "            W -= lr * W.grad\n",
    "            b -= lr * b.grad\n",
    "\n",
    "            # 기울기 초기화\n",
    "            W.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "\n",
    "    print(f'epoch {epoch + 1} : loss {loss.item():4f}')"
   ],
   "id": "1eb534729c35c24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : loss 7.654353\n",
      "epoch 2 : loss 3.781691\n",
      "epoch 3 : loss 0.975727\n",
      "epoch 4 : loss 0.370020\n",
      "epoch 5 : loss 0.231108\n",
      "epoch 6 : loss 0.184898\n",
      "epoch 7 : loss 0.187091\n",
      "epoch 8 : loss 0.106876\n",
      "epoch 9 : loss 0.244749\n",
      "epoch 10 : loss 0.143857\n",
      "epoch 11 : loss 0.226923\n",
      "epoch 12 : loss 0.150260\n",
      "epoch 13 : loss 0.227512\n",
      "epoch 14 : loss 0.240990\n",
      "epoch 15 : loss 0.167827\n",
      "epoch 16 : loss 0.328742\n",
      "epoch 17 : loss 0.119215\n",
      "epoch 18 : loss 0.318473\n",
      "epoch 19 : loss 0.307200\n",
      "epoch 20 : loss 0.298080\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T04:12:54.626928Z",
     "start_time": "2026-01-14T04:12:54.002263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. SGD 확률적 경사하강법\n",
    "# batch_size = 1인 미니배치 경사하강법\n",
    "W = torch.zeros(1, 2, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(20):\n",
    "    indices = torch.randperm(X.size(0)) # 데이터 샘플 수\n",
    "\n",
    "    for i in indices:\n",
    "        X_i = X[i]\n",
    "        y_i = y[i]\n",
    "\n",
    "        y_hat = X_i @ W.T + b\n",
    "        loss = torch.mean((y_hat - y_i) ** 2)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 기울기 갱신\n",
    "            W -= lr * W.grad\n",
    "            b -= lr * b.grad\n",
    "\n",
    "            # 기울기 초기화\n",
    "            W.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "\n",
    "    print(f'epoch {epoch + 1} : loss {loss.item():4f}')"
   ],
   "id": "3e4f3c3e3751c3cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : loss 0.178610\n",
      "epoch 2 : loss 0.059445\n",
      "epoch 3 : loss 0.219017\n",
      "epoch 4 : loss 0.236807\n",
      "epoch 5 : loss 0.025572\n",
      "epoch 6 : loss 0.010459\n",
      "epoch 7 : loss 0.478871\n",
      "epoch 8 : loss 0.168270\n",
      "epoch 9 : loss 0.605955\n",
      "epoch 10 : loss 0.238080\n",
      "epoch 11 : loss 0.034412\n",
      "epoch 12 : loss 0.020400\n",
      "epoch 13 : loss 0.306916\n",
      "epoch 14 : loss 0.192827\n",
      "epoch 15 : loss 0.445570\n",
      "epoch 16 : loss 1.325736\n",
      "epoch 17 : loss 0.561133\n",
      "epoch 18 : loss 0.161225\n",
      "epoch 19 : loss 0.433513\n",
      "epoch 20 : loss 0.064296\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataSet / DataLoader\n",
    "- BGD\n",
    "- Mini-batch GD\n",
    "- SGD"
   ],
   "id": "b7b2cd335d1b04cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T05:47:33.804871Z",
     "start_time": "2026-01-14T05:47:33.799741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터 생성\n",
    "X = torch.linspace(0, 10, 100).unsqueeze(1)\n",
    "y = 3 * X + 1 + torch.randn_like(X) * 0.5"
   ],
   "id": "86a65b8d59a48fdd",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T05:53:29.198627Z",
     "start_time": "2026-01-14T05:53:27.223451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터셋 클래스\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" 전체 데이터 수를 반환 \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "dataset = MyDataset(X, y)\n",
    "\n",
    "model = nn.Linear(1, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ],
   "id": "f439e858f1cb29b3",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T06:04:14.791601Z",
     "start_time": "2026-01-14T06:04:14.425764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. BGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=True)\n",
    "model = nn.Linear(1, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(200):\n",
    "    for X_batch, y_batch in dataloader: # 1번 반복\n",
    "        # print(X_batch, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} : Loss {loss.item():4f}')"
   ],
   "id": "1515aeda754e65f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss 267.474640\n",
      "Epoch 2 : Loss 265.303833\n",
      "Epoch 3 : Loss 263.142212\n",
      "Epoch 4 : Loss 260.989868\n",
      "Epoch 5 : Loss 258.846954\n",
      "Epoch 6 : Loss 256.713623\n",
      "Epoch 7 : Loss 254.590042\n",
      "Epoch 8 : Loss 252.476364\n",
      "Epoch 9 : Loss 250.372696\n",
      "Epoch 10 : Loss 248.279144\n",
      "Epoch 11 : Loss 246.195862\n",
      "Epoch 12 : Loss 244.123001\n",
      "Epoch 13 : Loss 242.060669\n",
      "Epoch 14 : Loss 240.008942\n",
      "Epoch 15 : Loss 237.967987\n",
      "Epoch 16 : Loss 235.937897\n",
      "Epoch 17 : Loss 233.918747\n",
      "Epoch 18 : Loss 231.910629\n",
      "Epoch 19 : Loss 229.913651\n",
      "Epoch 20 : Loss 227.927887\n",
      "Epoch 21 : Loss 225.953415\n",
      "Epoch 22 : Loss 223.990311\n",
      "Epoch 23 : Loss 222.038651\n",
      "Epoch 24 : Loss 220.098450\n",
      "Epoch 25 : Loss 218.169830\n",
      "Epoch 26 : Loss 216.252777\n",
      "Epoch 27 : Loss 214.347382\n",
      "Epoch 28 : Loss 212.453659\n",
      "Epoch 29 : Loss 210.571655\n",
      "Epoch 30 : Loss 208.701401\n",
      "Epoch 31 : Loss 206.842926\n",
      "Epoch 32 : Loss 204.996231\n",
      "Epoch 33 : Loss 203.161362\n",
      "Epoch 34 : Loss 201.338333\n",
      "Epoch 35 : Loss 199.527084\n",
      "Epoch 36 : Loss 197.727737\n",
      "Epoch 37 : Loss 195.940216\n",
      "Epoch 38 : Loss 194.164505\n",
      "Epoch 39 : Loss 192.400650\n",
      "Epoch 40 : Loss 190.648636\n",
      "Epoch 41 : Loss 188.908432\n",
      "Epoch 42 : Loss 187.180023\n",
      "Epoch 43 : Loss 185.463364\n",
      "Epoch 44 : Loss 183.758530\n",
      "Epoch 45 : Loss 182.065430\n",
      "Epoch 46 : Loss 180.384048\n",
      "Epoch 47 : Loss 178.714340\n",
      "Epoch 48 : Loss 177.056351\n",
      "Epoch 49 : Loss 175.409958\n",
      "Epoch 50 : Loss 173.775208\n",
      "Epoch 51 : Loss 172.152039\n",
      "Epoch 52 : Loss 170.540405\n",
      "Epoch 53 : Loss 168.940292\n",
      "Epoch 54 : Loss 167.351639\n",
      "Epoch 55 : Loss 165.774460\n",
      "Epoch 56 : Loss 164.208618\n",
      "Epoch 57 : Loss 162.654175\n",
      "Epoch 58 : Loss 161.111038\n",
      "Epoch 59 : Loss 159.579193\n",
      "Epoch 60 : Loss 158.058578\n",
      "Epoch 61 : Loss 156.549179\n",
      "Epoch 62 : Loss 155.050888\n",
      "Epoch 63 : Loss 153.563705\n",
      "Epoch 64 : Loss 152.087601\n",
      "Epoch 65 : Loss 150.622498\n",
      "Epoch 66 : Loss 149.168365\n",
      "Epoch 67 : Loss 147.725159\n",
      "Epoch 68 : Loss 146.292816\n",
      "Epoch 69 : Loss 144.871323\n",
      "Epoch 70 : Loss 143.460587\n",
      "Epoch 71 : Loss 142.060608\n",
      "Epoch 72 : Loss 140.671295\n",
      "Epoch 73 : Loss 139.292603\n",
      "Epoch 74 : Loss 137.924515\n",
      "Epoch 75 : Loss 136.566956\n",
      "Epoch 76 : Loss 135.219910\n",
      "Epoch 77 : Loss 133.883286\n",
      "Epoch 78 : Loss 132.557053\n",
      "Epoch 79 : Loss 131.241150\n",
      "Epoch 80 : Loss 129.935547\n",
      "Epoch 81 : Loss 128.640182\n",
      "Epoch 82 : Loss 127.355011\n",
      "Epoch 83 : Loss 126.079979\n",
      "Epoch 84 : Loss 124.815041\n",
      "Epoch 85 : Loss 123.560135\n",
      "Epoch 86 : Loss 122.315216\n",
      "Epoch 87 : Loss 121.080223\n",
      "Epoch 88 : Loss 119.855156\n",
      "Epoch 89 : Loss 118.639900\n",
      "Epoch 90 : Loss 117.434456\n",
      "Epoch 91 : Loss 116.238747\n",
      "Epoch 92 : Loss 115.052711\n",
      "Epoch 93 : Loss 113.876320\n",
      "Epoch 94 : Loss 112.709511\n",
      "Epoch 95 : Loss 111.552246\n",
      "Epoch 96 : Loss 110.404449\n",
      "Epoch 97 : Loss 109.266106\n",
      "Epoch 98 : Loss 108.137146\n",
      "Epoch 99 : Loss 107.017517\n",
      "Epoch 100 : Loss 105.907181\n",
      "Epoch 101 : Loss 104.806061\n",
      "Epoch 102 : Loss 103.714142\n",
      "Epoch 103 : Loss 102.631348\n",
      "Epoch 104 : Loss 101.557648\n",
      "Epoch 105 : Loss 100.492966\n",
      "Epoch 106 : Loss 99.437286\n",
      "Epoch 107 : Loss 98.390526\n",
      "Epoch 108 : Loss 97.352669\n",
      "Epoch 109 : Loss 96.323616\n",
      "Epoch 110 : Loss 95.303360\n",
      "Epoch 111 : Loss 94.291847\n",
      "Epoch 112 : Loss 93.288994\n",
      "Epoch 113 : Loss 92.294777\n",
      "Epoch 114 : Loss 91.309158\n",
      "Epoch 115 : Loss 90.332069\n",
      "Epoch 116 : Loss 89.363457\n",
      "Epoch 117 : Loss 88.403297\n",
      "Epoch 118 : Loss 87.451523\n",
      "Epoch 119 : Loss 86.508080\n",
      "Epoch 120 : Loss 85.572922\n",
      "Epoch 121 : Loss 84.645996\n",
      "Epoch 122 : Loss 83.727264\n",
      "Epoch 123 : Loss 82.816658\n",
      "Epoch 124 : Loss 81.914162\n",
      "Epoch 125 : Loss 81.019699\n",
      "Epoch 126 : Loss 80.133224\n",
      "Epoch 127 : Loss 79.254700\n",
      "Epoch 128 : Loss 78.384071\n",
      "Epoch 129 : Loss 77.521286\n",
      "Epoch 130 : Loss 76.666298\n",
      "Epoch 131 : Loss 75.819061\n",
      "Epoch 132 : Loss 74.979523\n",
      "Epoch 133 : Loss 74.147644\n",
      "Epoch 134 : Loss 73.323372\n",
      "Epoch 135 : Loss 72.506653\n",
      "Epoch 136 : Loss 71.697441\n",
      "Epoch 137 : Loss 70.895691\n",
      "Epoch 138 : Loss 70.101357\n",
      "Epoch 139 : Loss 69.314400\n",
      "Epoch 140 : Loss 68.534752\n",
      "Epoch 141 : Loss 67.762375\n",
      "Epoch 142 : Loss 66.997223\n",
      "Epoch 143 : Loss 66.239243\n",
      "Epoch 144 : Loss 65.488388\n",
      "Epoch 145 : Loss 64.744629\n",
      "Epoch 146 : Loss 64.007889\n",
      "Epoch 147 : Loss 63.278141\n",
      "Epoch 148 : Loss 62.555332\n",
      "Epoch 149 : Loss 61.839420\n",
      "Epoch 150 : Loss 61.130360\n",
      "Epoch 151 : Loss 60.428097\n",
      "Epoch 152 : Loss 59.732601\n",
      "Epoch 153 : Loss 59.043808\n",
      "Epoch 154 : Loss 58.361671\n",
      "Epoch 155 : Loss 57.686161\n",
      "Epoch 156 : Loss 57.017231\n",
      "Epoch 157 : Loss 56.354813\n",
      "Epoch 158 : Loss 55.698887\n",
      "Epoch 159 : Loss 55.049393\n",
      "Epoch 160 : Loss 54.406288\n",
      "Epoch 161 : Loss 53.769531\n",
      "Epoch 162 : Loss 53.139069\n",
      "Epoch 163 : Loss 52.514862\n",
      "Epoch 164 : Loss 51.896866\n",
      "Epoch 165 : Loss 51.285046\n",
      "Epoch 166 : Loss 50.679337\n",
      "Epoch 167 : Loss 50.079708\n",
      "Epoch 168 : Loss 49.486115\n",
      "Epoch 169 : Loss 48.898502\n",
      "Epoch 170 : Loss 48.316841\n",
      "Epoch 171 : Loss 47.741077\n",
      "Epoch 172 : Loss 47.171177\n",
      "Epoch 173 : Loss 46.607090\n",
      "Epoch 174 : Loss 46.048771\n",
      "Epoch 175 : Loss 45.496181\n",
      "Epoch 176 : Loss 44.949276\n",
      "Epoch 177 : Loss 44.408016\n",
      "Epoch 178 : Loss 43.872353\n",
      "Epoch 179 : Loss 43.342239\n",
      "Epoch 180 : Loss 42.817646\n",
      "Epoch 181 : Loss 42.298534\n",
      "Epoch 182 : Loss 41.784840\n",
      "Epoch 183 : Loss 41.276539\n",
      "Epoch 184 : Loss 40.773575\n",
      "Epoch 185 : Loss 40.275917\n",
      "Epoch 186 : Loss 39.783524\n",
      "Epoch 187 : Loss 39.296349\n",
      "Epoch 188 : Loss 38.814354\n",
      "Epoch 189 : Loss 38.337494\n",
      "Epoch 190 : Loss 37.865734\n",
      "Epoch 191 : Loss 37.399033\n",
      "Epoch 192 : Loss 36.937344\n",
      "Epoch 193 : Loss 36.480640\n",
      "Epoch 194 : Loss 36.028866\n",
      "Epoch 195 : Loss 35.581985\n",
      "Epoch 196 : Loss 35.139961\n",
      "Epoch 197 : Loss 34.702751\n",
      "Epoch 198 : Loss 34.270321\n",
      "Epoch 199 : Loss 33.842625\n",
      "Epoch 200 : Loss 33.419632\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T06:05:55.321406Z",
     "start_time": "2026-01-14T06:05:54.197226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Mini-batch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True : 마지막 배치개수가 지정한 batch_size보다 작은 경우 버림\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, drop_last=True)\n",
    "model = nn.Linear(1, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(200):\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        # print(len(X_batch))\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} : Loss {loss.item():4f}')"
   ],
   "id": "2253517359a61c22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss 335.335815\n",
      "Epoch 2 : Loss 251.413635\n",
      "Epoch 3 : Loss 400.676025\n",
      "Epoch 4 : Loss 282.003571\n",
      "Epoch 5 : Loss 411.072327\n",
      "Epoch 6 : Loss 234.573669\n",
      "Epoch 7 : Loss 237.292877\n",
      "Epoch 8 : Loss 181.883881\n",
      "Epoch 9 : Loss 214.621262\n",
      "Epoch 10 : Loss 235.405090\n",
      "Epoch 11 : Loss 209.465027\n",
      "Epoch 12 : Loss 200.349472\n",
      "Epoch 13 : Loss 159.858429\n",
      "Epoch 14 : Loss 197.227417\n",
      "Epoch 15 : Loss 144.412827\n",
      "Epoch 16 : Loss 172.036331\n",
      "Epoch 17 : Loss 159.678772\n",
      "Epoch 18 : Loss 116.573700\n",
      "Epoch 19 : Loss 163.350845\n",
      "Epoch 20 : Loss 121.464325\n",
      "Epoch 21 : Loss 133.501816\n",
      "Epoch 22 : Loss 159.879120\n",
      "Epoch 23 : Loss 82.365341\n",
      "Epoch 24 : Loss 63.984837\n",
      "Epoch 25 : Loss 113.877731\n",
      "Epoch 26 : Loss 103.633469\n",
      "Epoch 27 : Loss 100.683121\n",
      "Epoch 28 : Loss 82.109497\n",
      "Epoch 29 : Loss 97.637138\n",
      "Epoch 30 : Loss 72.123558\n",
      "Epoch 31 : Loss 58.397919\n",
      "Epoch 32 : Loss 51.428051\n",
      "Epoch 33 : Loss 57.510666\n",
      "Epoch 34 : Loss 37.720303\n",
      "Epoch 35 : Loss 54.898708\n",
      "Epoch 36 : Loss 41.197731\n",
      "Epoch 37 : Loss 48.757271\n",
      "Epoch 38 : Loss 52.450806\n",
      "Epoch 39 : Loss 36.692970\n",
      "Epoch 40 : Loss 24.606609\n",
      "Epoch 41 : Loss 44.785839\n",
      "Epoch 42 : Loss 26.943743\n",
      "Epoch 43 : Loss 27.404257\n",
      "Epoch 44 : Loss 30.504913\n",
      "Epoch 45 : Loss 19.518780\n",
      "Epoch 46 : Loss 21.009153\n",
      "Epoch 47 : Loss 29.473623\n",
      "Epoch 48 : Loss 12.210741\n",
      "Epoch 49 : Loss 11.841840\n",
      "Epoch 50 : Loss 25.339113\n",
      "Epoch 51 : Loss 28.959743\n",
      "Epoch 52 : Loss 15.348501\n",
      "Epoch 53 : Loss 24.050770\n",
      "Epoch 54 : Loss 6.108522\n",
      "Epoch 55 : Loss 20.857380\n",
      "Epoch 56 : Loss 12.729425\n",
      "Epoch 57 : Loss 7.766102\n",
      "Epoch 58 : Loss 9.112383\n",
      "Epoch 59 : Loss 5.117365\n",
      "Epoch 60 : Loss 9.154952\n",
      "Epoch 61 : Loss 8.091596\n",
      "Epoch 62 : Loss 4.176825\n",
      "Epoch 63 : Loss 8.799333\n",
      "Epoch 64 : Loss 9.722622\n",
      "Epoch 65 : Loss 6.084347\n",
      "Epoch 66 : Loss 6.097991\n",
      "Epoch 67 : Loss 5.923877\n",
      "Epoch 68 : Loss 4.955569\n",
      "Epoch 69 : Loss 3.414121\n",
      "Epoch 70 : Loss 4.965599\n",
      "Epoch 71 : Loss 5.091966\n",
      "Epoch 72 : Loss 4.378176\n",
      "Epoch 73 : Loss 4.373693\n",
      "Epoch 74 : Loss 3.913385\n",
      "Epoch 75 : Loss 2.107622\n",
      "Epoch 76 : Loss 3.014897\n",
      "Epoch 77 : Loss 3.998785\n",
      "Epoch 78 : Loss 2.896532\n",
      "Epoch 79 : Loss 3.378751\n",
      "Epoch 80 : Loss 2.172202\n",
      "Epoch 81 : Loss 2.685524\n",
      "Epoch 82 : Loss 2.553098\n",
      "Epoch 83 : Loss 1.677326\n",
      "Epoch 84 : Loss 2.743395\n",
      "Epoch 85 : Loss 2.941388\n",
      "Epoch 86 : Loss 1.665087\n",
      "Epoch 87 : Loss 1.096399\n",
      "Epoch 88 : Loss 3.268604\n",
      "Epoch 89 : Loss 2.149915\n",
      "Epoch 90 : Loss 1.487113\n",
      "Epoch 91 : Loss 2.061674\n",
      "Epoch 92 : Loss 1.866644\n",
      "Epoch 93 : Loss 1.238616\n",
      "Epoch 94 : Loss 1.463337\n",
      "Epoch 95 : Loss 1.437851\n",
      "Epoch 96 : Loss 2.229957\n",
      "Epoch 97 : Loss 1.403659\n",
      "Epoch 98 : Loss 1.917104\n",
      "Epoch 99 : Loss 0.999221\n",
      "Epoch 100 : Loss 1.578321\n",
      "Epoch 101 : Loss 2.055904\n",
      "Epoch 102 : Loss 1.795973\n",
      "Epoch 103 : Loss 1.842989\n",
      "Epoch 104 : Loss 1.305624\n",
      "Epoch 105 : Loss 1.447918\n",
      "Epoch 106 : Loss 1.597909\n",
      "Epoch 107 : Loss 1.487690\n",
      "Epoch 108 : Loss 1.481942\n",
      "Epoch 109 : Loss 1.525928\n",
      "Epoch 110 : Loss 1.209879\n",
      "Epoch 111 : Loss 0.949074\n",
      "Epoch 112 : Loss 1.887192\n",
      "Epoch 113 : Loss 1.771705\n",
      "Epoch 114 : Loss 1.564693\n",
      "Epoch 115 : Loss 1.549909\n",
      "Epoch 116 : Loss 1.402133\n",
      "Epoch 117 : Loss 1.620420\n",
      "Epoch 118 : Loss 1.252167\n",
      "Epoch 119 : Loss 0.990145\n",
      "Epoch 120 : Loss 1.355166\n",
      "Epoch 121 : Loss 1.650647\n",
      "Epoch 122 : Loss 1.539460\n",
      "Epoch 123 : Loss 1.001396\n",
      "Epoch 124 : Loss 1.736559\n",
      "Epoch 125 : Loss 0.987819\n",
      "Epoch 126 : Loss 1.475248\n",
      "Epoch 127 : Loss 1.266671\n",
      "Epoch 128 : Loss 1.351263\n",
      "Epoch 129 : Loss 1.138875\n",
      "Epoch 130 : Loss 1.739897\n",
      "Epoch 131 : Loss 1.133266\n",
      "Epoch 132 : Loss 1.220867\n",
      "Epoch 133 : Loss 1.061266\n",
      "Epoch 134 : Loss 1.994939\n",
      "Epoch 135 : Loss 1.137522\n",
      "Epoch 136 : Loss 1.520876\n",
      "Epoch 137 : Loss 0.964797\n",
      "Epoch 138 : Loss 1.689929\n",
      "Epoch 139 : Loss 1.228453\n",
      "Epoch 140 : Loss 1.506489\n",
      "Epoch 141 : Loss 1.824532\n",
      "Epoch 142 : Loss 0.915599\n",
      "Epoch 143 : Loss 0.644517\n",
      "Epoch 144 : Loss 1.175151\n",
      "Epoch 145 : Loss 0.846503\n",
      "Epoch 146 : Loss 1.148144\n",
      "Epoch 147 : Loss 1.167450\n",
      "Epoch 148 : Loss 1.211201\n",
      "Epoch 149 : Loss 1.370459\n",
      "Epoch 150 : Loss 1.955755\n",
      "Epoch 151 : Loss 1.258290\n",
      "Epoch 152 : Loss 1.169352\n",
      "Epoch 153 : Loss 1.405216\n",
      "Epoch 154 : Loss 0.999156\n",
      "Epoch 155 : Loss 1.527995\n",
      "Epoch 156 : Loss 1.512957\n",
      "Epoch 157 : Loss 1.000577\n",
      "Epoch 158 : Loss 1.001983\n",
      "Epoch 159 : Loss 0.954376\n",
      "Epoch 160 : Loss 1.228344\n",
      "Epoch 161 : Loss 0.949011\n",
      "Epoch 162 : Loss 1.467523\n",
      "Epoch 163 : Loss 0.570920\n",
      "Epoch 164 : Loss 0.960275\n",
      "Epoch 165 : Loss 0.830078\n",
      "Epoch 166 : Loss 1.204066\n",
      "Epoch 167 : Loss 0.882264\n",
      "Epoch 168 : Loss 1.149780\n",
      "Epoch 169 : Loss 1.484662\n",
      "Epoch 170 : Loss 0.666791\n",
      "Epoch 171 : Loss 1.012846\n",
      "Epoch 172 : Loss 1.276012\n",
      "Epoch 173 : Loss 1.229545\n",
      "Epoch 174 : Loss 0.921767\n",
      "Epoch 175 : Loss 1.043709\n",
      "Epoch 176 : Loss 0.990815\n",
      "Epoch 177 : Loss 1.202699\n",
      "Epoch 178 : Loss 1.037585\n",
      "Epoch 179 : Loss 1.058741\n",
      "Epoch 180 : Loss 1.328708\n",
      "Epoch 181 : Loss 0.558404\n",
      "Epoch 182 : Loss 1.467508\n",
      "Epoch 183 : Loss 1.094108\n",
      "Epoch 184 : Loss 0.899145\n",
      "Epoch 185 : Loss 0.570165\n",
      "Epoch 186 : Loss 1.098061\n",
      "Epoch 187 : Loss 0.989702\n",
      "Epoch 188 : Loss 1.364431\n",
      "Epoch 189 : Loss 0.574044\n",
      "Epoch 190 : Loss 0.770181\n",
      "Epoch 191 : Loss 0.763406\n",
      "Epoch 192 : Loss 0.912590\n",
      "Epoch 193 : Loss 0.504584\n",
      "Epoch 194 : Loss 0.946047\n",
      "Epoch 195 : Loss 1.056621\n",
      "Epoch 196 : Loss 1.356764\n",
      "Epoch 197 : Loss 1.193230\n",
      "Epoch 198 : Loss 0.990850\n",
      "Epoch 199 : Loss 0.849307\n",
      "Epoch 200 : Loss 0.859278\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T06:25:25.994031Z",
     "start_time": "2026-01-14T06:25:11.846934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "model = nn.Linear(1, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(200):\n",
    "    for X_batch, y_batch in dataloader: # 100 / 16번만큼 반복\n",
    "        # print(len(X_batch))\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} : Loss {loss.item():4f}')"
   ],
   "id": "4d5e73045050d4b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Loss 120.875282\n",
      "Epoch 2 : Loss 80.618980\n",
      "Epoch 3 : Loss 255.737335\n",
      "Epoch 4 : Loss 0.664807\n",
      "Epoch 5 : Loss 13.602074\n",
      "Epoch 6 : Loss 50.421680\n",
      "Epoch 7 : Loss 22.417576\n",
      "Epoch 8 : Loss 11.511048\n",
      "Epoch 9 : Loss 2.808770\n",
      "Epoch 10 : Loss 0.517784\n",
      "Epoch 11 : Loss 0.354660\n",
      "Epoch 12 : Loss 0.280270\n",
      "Epoch 13 : Loss 1.427498\n",
      "Epoch 14 : Loss 2.517770\n",
      "Epoch 15 : Loss 3.409373\n",
      "Epoch 16 : Loss 0.227747\n",
      "Epoch 17 : Loss 1.031080\n",
      "Epoch 18 : Loss 0.097234\n",
      "Epoch 19 : Loss 3.088644\n",
      "Epoch 20 : Loss 0.005573\n",
      "Epoch 21 : Loss 0.412443\n",
      "Epoch 22 : Loss 0.031193\n",
      "Epoch 23 : Loss 0.241918\n",
      "Epoch 24 : Loss 2.223295\n",
      "Epoch 25 : Loss 0.871337\n",
      "Epoch 26 : Loss 0.240564\n",
      "Epoch 27 : Loss 3.341877\n",
      "Epoch 28 : Loss 1.481860\n",
      "Epoch 29 : Loss 2.124819\n",
      "Epoch 30 : Loss 0.281556\n",
      "Epoch 31 : Loss 0.000369\n",
      "Epoch 32 : Loss 0.077966\n",
      "Epoch 33 : Loss 0.781459\n",
      "Epoch 34 : Loss 1.724815\n",
      "Epoch 35 : Loss 1.022096\n",
      "Epoch 36 : Loss 0.120773\n",
      "Epoch 37 : Loss 0.000389\n",
      "Epoch 38 : Loss 0.054404\n",
      "Epoch 39 : Loss 0.058295\n",
      "Epoch 40 : Loss 0.106681\n",
      "Epoch 41 : Loss 0.331284\n",
      "Epoch 42 : Loss 0.670349\n",
      "Epoch 43 : Loss 1.359530\n",
      "Epoch 44 : Loss 0.133269\n",
      "Epoch 45 : Loss 1.007043\n",
      "Epoch 46 : Loss 0.104210\n",
      "Epoch 47 : Loss 0.052453\n",
      "Epoch 48 : Loss 0.302166\n",
      "Epoch 49 : Loss 0.642391\n",
      "Epoch 50 : Loss 0.013702\n",
      "Epoch 51 : Loss 0.497321\n",
      "Epoch 52 : Loss 0.152371\n",
      "Epoch 53 : Loss 0.003906\n",
      "Epoch 54 : Loss 0.070878\n",
      "Epoch 55 : Loss 0.006967\n",
      "Epoch 56 : Loss 0.241663\n",
      "Epoch 57 : Loss 0.005841\n",
      "Epoch 58 : Loss 0.040578\n",
      "Epoch 59 : Loss 2.929781\n",
      "Epoch 60 : Loss 0.010020\n",
      "Epoch 61 : Loss 0.003571\n",
      "Epoch 62 : Loss 0.010289\n",
      "Epoch 63 : Loss 0.165582\n",
      "Epoch 64 : Loss 0.085732\n",
      "Epoch 65 : Loss 0.459301\n",
      "Epoch 66 : Loss 0.493417\n",
      "Epoch 67 : Loss 0.480422\n",
      "Epoch 68 : Loss 0.065914\n",
      "Epoch 69 : Loss 0.006277\n",
      "Epoch 70 : Loss 0.896250\n",
      "Epoch 71 : Loss 0.017808\n",
      "Epoch 72 : Loss 0.035980\n",
      "Epoch 73 : Loss 0.117359\n",
      "Epoch 74 : Loss 0.658878\n",
      "Epoch 75 : Loss 0.000853\n",
      "Epoch 76 : Loss 0.493393\n",
      "Epoch 77 : Loss 0.003402\n",
      "Epoch 78 : Loss 0.087939\n",
      "Epoch 79 : Loss 0.089247\n",
      "Epoch 80 : Loss 0.050867\n",
      "Epoch 81 : Loss 0.021301\n",
      "Epoch 82 : Loss 0.289672\n",
      "Epoch 83 : Loss 0.939552\n",
      "Epoch 84 : Loss 0.094835\n",
      "Epoch 85 : Loss 0.275643\n",
      "Epoch 86 : Loss 0.224241\n",
      "Epoch 87 : Loss 0.006948\n",
      "Epoch 88 : Loss 0.052789\n",
      "Epoch 89 : Loss 0.009019\n",
      "Epoch 90 : Loss 0.001524\n",
      "Epoch 91 : Loss 0.047607\n",
      "Epoch 92 : Loss 0.314234\n",
      "Epoch 93 : Loss 0.173962\n",
      "Epoch 94 : Loss 0.500125\n",
      "Epoch 95 : Loss 0.023883\n",
      "Epoch 96 : Loss 0.477621\n",
      "Epoch 97 : Loss 0.073571\n",
      "Epoch 98 : Loss 0.092659\n",
      "Epoch 99 : Loss 0.046969\n",
      "Epoch 100 : Loss 0.158678\n",
      "Epoch 101 : Loss 0.280028\n",
      "Epoch 102 : Loss 0.143688\n",
      "Epoch 103 : Loss 0.198408\n",
      "Epoch 104 : Loss 0.010603\n",
      "Epoch 105 : Loss 0.120430\n",
      "Epoch 106 : Loss 0.319092\n",
      "Epoch 107 : Loss 0.173082\n",
      "Epoch 108 : Loss 0.204415\n",
      "Epoch 109 : Loss 0.084358\n",
      "Epoch 110 : Loss 0.094738\n",
      "Epoch 111 : Loss 0.120489\n",
      "Epoch 112 : Loss 0.243651\n",
      "Epoch 113 : Loss 0.415632\n",
      "Epoch 114 : Loss 0.118982\n",
      "Epoch 115 : Loss 0.304732\n",
      "Epoch 116 : Loss 0.095687\n",
      "Epoch 117 : Loss 0.529000\n",
      "Epoch 118 : Loss 0.037591\n",
      "Epoch 119 : Loss 0.239926\n",
      "Epoch 120 : Loss 0.064722\n",
      "Epoch 121 : Loss 0.081463\n",
      "Epoch 122 : Loss 0.031761\n",
      "Epoch 123 : Loss 0.375774\n",
      "Epoch 124 : Loss 0.001212\n",
      "Epoch 125 : Loss 0.187532\n",
      "Epoch 126 : Loss 0.001803\n",
      "Epoch 127 : Loss 0.010748\n",
      "Epoch 128 : Loss 0.030124\n",
      "Epoch 129 : Loss 0.495775\n",
      "Epoch 130 : Loss 0.214731\n",
      "Epoch 131 : Loss 0.001468\n",
      "Epoch 132 : Loss 0.258099\n",
      "Epoch 133 : Loss 0.598328\n",
      "Epoch 134 : Loss 0.001296\n",
      "Epoch 135 : Loss 0.000047\n",
      "Epoch 136 : Loss 0.027825\n",
      "Epoch 137 : Loss 0.011644\n",
      "Epoch 138 : Loss 0.012033\n",
      "Epoch 139 : Loss 0.031238\n",
      "Epoch 140 : Loss 0.315799\n",
      "Epoch 141 : Loss 0.003413\n",
      "Epoch 142 : Loss 0.115677\n",
      "Epoch 143 : Loss 0.069305\n",
      "Epoch 144 : Loss 0.000800\n",
      "Epoch 145 : Loss 0.275254\n",
      "Epoch 146 : Loss 0.088205\n",
      "Epoch 147 : Loss 0.284247\n",
      "Epoch 148 : Loss 0.001354\n",
      "Epoch 149 : Loss 0.014377\n",
      "Epoch 150 : Loss 0.013378\n",
      "Epoch 151 : Loss 0.574826\n",
      "Epoch 152 : Loss 0.058728\n",
      "Epoch 153 : Loss 0.296737\n",
      "Epoch 154 : Loss 0.036693\n",
      "Epoch 155 : Loss 0.339845\n",
      "Epoch 156 : Loss 0.001382\n",
      "Epoch 157 : Loss 0.042403\n",
      "Epoch 158 : Loss 0.761295\n",
      "Epoch 159 : Loss 0.013384\n",
      "Epoch 160 : Loss 0.335370\n",
      "Epoch 161 : Loss 0.034618\n",
      "Epoch 162 : Loss 0.000920\n",
      "Epoch 163 : Loss 0.065161\n",
      "Epoch 164 : Loss 0.765278\n",
      "Epoch 165 : Loss 0.045385\n",
      "Epoch 166 : Loss 0.474977\n",
      "Epoch 167 : Loss 0.098158\n",
      "Epoch 168 : Loss 0.014890\n",
      "Epoch 169 : Loss 0.000014\n",
      "Epoch 170 : Loss 0.156058\n",
      "Epoch 171 : Loss 0.086249\n",
      "Epoch 172 : Loss 0.000759\n",
      "Epoch 173 : Loss 0.034864\n",
      "Epoch 174 : Loss 0.017083\n",
      "Epoch 175 : Loss 0.098066\n",
      "Epoch 176 : Loss 0.008905\n",
      "Epoch 177 : Loss 0.029515\n",
      "Epoch 178 : Loss 0.018894\n",
      "Epoch 179 : Loss 0.027608\n",
      "Epoch 180 : Loss 0.211140\n",
      "Epoch 181 : Loss 0.145772\n",
      "Epoch 182 : Loss 0.061817\n",
      "Epoch 183 : Loss 0.002400\n",
      "Epoch 184 : Loss 0.020489\n",
      "Epoch 185 : Loss 0.147647\n",
      "Epoch 186 : Loss 0.043434\n",
      "Epoch 187 : Loss 0.142228\n",
      "Epoch 188 : Loss 0.026830\n",
      "Epoch 189 : Loss 0.003631\n",
      "Epoch 190 : Loss 0.309443\n",
      "Epoch 191 : Loss 0.018939\n",
      "Epoch 192 : Loss 0.020317\n",
      "Epoch 193 : Loss 0.010559\n",
      "Epoch 194 : Loss 0.008221\n",
      "Epoch 195 : Loss 0.089715\n",
      "Epoch 196 : Loss 0.465866\n",
      "Epoch 197 : Loss 0.571445\n",
      "Epoch 198 : Loss 0.018339\n",
      "Epoch 199 : Loss 0.074711\n",
      "Epoch 200 : Loss 0.000006\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 손글씨 분류 MNIST",
   "id": "8f6b2687919fa9fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T01:36:14.553495Z",
     "start_time": "2026-01-15T01:36:14.492032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터 로드\n",
    "# 깃이그노어에 추가 : /**/MNIST/raw\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), #ndarray, PIL이미지 타입등을 tensor타입으로 변환(0~1 스케일링)\n",
    "    # (픽셀값 - 평균) / 표준편차 -> 값의 범위를 -1 ~ 1\n",
    "    transforms.Normalize((0.5), (0.5)) # 입력데이터(0~1)에 대한 정규화 (평균0, 표준편차1)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 평가/검증 데이터는 순서를 섞지 않는다.\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "709f873af35d5ae8",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T01:36:17.114840Z",
     "start_time": "2026-01-15T01:36:17.080091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터 확인\n",
    "print(len(train_dataset)) # 60000\n",
    "print(len(test_dataset)) # 10000\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    image = images[0]\n",
    "    image= image.flatten()\n",
    "    print(image.shape)\n",
    "    print(labels[:10])\n",
    "    break"
   ],
   "id": "82ec28490f071f56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "torch.Size([784])\n",
      "tensor([6, 0, 2, 8, 8, 3, 7, 6, 2, 7])\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T01:51:01.311294Z",
     "start_time": "2026-01-15T01:51:01.300905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 모델 생성\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 128), # 정형데이터로 처리할려고 함\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MNISTNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "dc6fc082eb33fa2b",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T01:53:47.302444Z",
     "start_time": "2026-01-15T01:51:03.432513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 모델 학습\n",
    "model.train() # training = True 기본값\n",
    "print(model.training)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} : Loss {loss.item():.4f}')\n"
   ],
   "id": "6f4a773e4378dc57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 1 : Loss 0.4275\n",
      "Epoch 2 : Loss 0.1227\n",
      "Epoch 3 : Loss 0.2348\n",
      "Epoch 4 : Loss 0.1816\n",
      "Epoch 5 : Loss 0.0544\n",
      "Epoch 6 : Loss 0.0159\n",
      "Epoch 7 : Loss 0.0063\n",
      "Epoch 8 : Loss 0.0029\n",
      "Epoch 9 : Loss 0.0158\n",
      "Epoch 10 : Loss 0.1127\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T01:58:50.112350Z",
     "start_time": "2026-01-15T01:58:44.026898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "print(model.training) # False\n",
    "\n",
    "all_preds = []\n",
    "all_y = []\n",
    "all_X = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        logits = model(X_batch)\n",
    "        p = F.softmax(logits, dim=1)\n",
    "        pred = p.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(pred)\n",
    "        all_X.extend(X_batch)\n",
    "        all_y.extend(y_batch)\n",
    "\n",
    "print(len(all_preds), len(all_X), len(all_y))\n",
    "print('정확도 : ', accuracy_score(all_y, all_preds))"
   ],
   "id": "1cd5156568da7f2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "10000 10000 10000\n",
      "정확도 :  0.9714\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T02:06:36.988441Z",
     "start_time": "2026-01-15T02:06:36.270514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 시각화\n",
    "num_images = 10\n",
    "indices = np.random.choice(len(all_X), num_images, replace=False) # 비복원 추출\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    image = all_X[idx].squeeze()\n",
    "    label = all_y[idx]\n",
    "    pred = all_preds[idx]\n",
    "\n",
    "    plt.subplot(1, num_images, i + 1)\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    plt.title(f'Label : {label}\\nPred : {pred}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "id": "693026b750fe8e27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T02:09:57.071112Z",
     "start_time": "2026-01-15T02:09:56.604579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 예측이 틀린 손글씨 시각화\n",
    "\n",
    "num_images = 10\n",
    "indices = [i for i in range(len(all_X)) if all_preds[i] != all_y[i]]\n",
    "indices = indices[:num_images]\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i, idx in enumerate(indices):\n",
    "    image = all_X[idx].squeeze()\n",
    "    label = all_y[idx]\n",
    "    pred = all_preds[idx]\n",
    "\n",
    "    plt.subplot(1, num_images, i + 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f'Label : {label}\\nPred : {pred}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "id": "3c9f4aacf7875049",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 86
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
